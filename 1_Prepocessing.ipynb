{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase.\n",
      "Teks setelah diubah menjadi lowercase: ini adalah contoh teks yang akan dikonversi menjadi lowercase.\n"
     ]
    }
   ],
   "source": [
    "# Contoh teks\n",
    "teks_asli = \"Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase.\"\n",
    " \n",
    "# Mengubah teks menjadi lowercase\n",
    "teks_lowercase = teks_asli.lower()\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\", teks_asli)\n",
    "print(\"Teks setelah diubah menjadi lowercase:\", teks_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks dengan angka: Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\n",
      "Teks tanpa angka: Ini adalah contoh teks dengan angka  yang akan dihapus.\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk menghapus angka dari teks\n",
    "def hapus_angka(teks):\n",
    "    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])\n",
    "    return teks_tanpa_angka\n",
    " \n",
    "# Contoh teks dengan angka\n",
    "teks_dengan_angka = \"Ini adalah contoh teks dengan angka 12345 yang akan dihapus.\"\n",
    " \n",
    "# Memanggil fungsi untuk menghapus angka\n",
    "teks_tanpa_angka = hapus_angka(teks_dengan_angka)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks dengan angka:\", teks_dengan_angka)\n",
    "print(\"Teks tanpa angka:\", teks_tanpa_angka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat dengan angka: Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut.\n",
      "Kalimat tanpa angka tidak relevan: Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi  untuk informasi lebih lanjut.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "def hapus_angka_tidak_relevan(teks):\n",
    "    # Menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan\n",
    "    # Pola untuk mengenali angka yang harus dihapus, termasuk nomor rumah dan nomor telepon\n",
    "    pola_angka_tidak_relevan = r\"\\b(?:\\d{1,3}[-\\.\\s]?)?(?:\\d{3}[-\\.\\s]?)?\\d{4,}\\b\"\n",
    "    hasil = re.sub(pola_angka_tidak_relevan, \"\", teks)\n",
    "    return hasil.strip()\n",
    " \n",
    "# Contoh kalimat dengan angka\n",
    "kalimat = \"Di sini ada 3 nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut.\"\n",
    " \n",
    "# Memanggil fungsi untuk menghapus angka tidak relevan\n",
    "hasil_tanpa_angka = hapus_angka_tidak_relevan(kalimat)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Kalimat dengan angka:\", kalimat)\n",
    "print(\"Kalimat tanpa angka tidak relevan:\", hasil_tanpa_angka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\n",
      "Teks setelah menghapus tanda baca: Ini adalah contoh teks dengan tanda baca Contoh ini digunakan untuk demonstrasi\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    " \n",
    "def remove_punctuation(text):\n",
    "    # Membuat set yang berisi semua tanda baca\n",
    "    punctuation_set = set(string.punctuation)\n",
    " \n",
    "    # Menghapus tanda baca dari teks\n",
    "    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
    " \n",
    "    return text_without_punctuation\n",
    " \n",
    "# Contoh teks dengan tanda baca\n",
    "teks_asli = \"Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi.\"\n",
    " \n",
    "# Menghapus tanda baca dari teks\n",
    "teks_tanpa_tanda_baca = remove_punctuation(teks_asli)\n",
    " \n",
    "# Menampilkan hasil\n",
    "print(\"Teks asli:\", teks_asli)\n",
    "print(\"Teks setelah menghapus tanda baca:\", teks_tanpa_tanda_baca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah contoh kalimat dengan spasi di awal dan akhir.\n"
     ]
    }
   ],
   "source": [
    "teks = \"   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    \"\n",
    "teks_setelah_strip = teks.strip()\n",
    "print(teks_setelah_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniadalahcontohkalimatdenganspasididalamnya.\n"
     ]
    }
   ],
   "source": [
    "teks_dengan_whitespace = \"Ini adalah    contoh kalimat    dengan spasi    di dalamnya.\"\n",
    "teks_tanpa_whitespace = teks_dengan_whitespace.replace(\" \", \"\")\n",
    "print(teks_tanpa_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords NLTK: Perekonomian Indonesia pertumbuhan membanggakan .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pongo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pongo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # Untuk tokenisasi kata\n",
    " \n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    " \n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    " \n",
    "# Ambil daftar stopwords bahasa Indonesia dari NLTK\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    " \n",
    "# Filtering kata-kata dengan menghapus stopwords\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]\n",
    " \n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    " \n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords NLTK:\", teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords Sastrawi: Perekonomian Indonesia sedang pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "source": [
    "# Install Sastrawi library if not already installed\n",
    " \n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# Inisialisasi objek StopWordRemover dari Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    " \n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    " \n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    " \n",
    "# Filtering kata-kata dengan menghapus stopwords Sastrawi\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
    " \n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    " \n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords Sastrawi:\", teks_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini adalah contoh tokenisasi kalimat.', 'Apakah ini kalimat kedua?', 'Ya, ini kalimat ketiga!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pemrosesan', 'teks', 'adalah', 'cabang', 'ilmu', 'komputer', 'yang', 'berfokus', 'pada', 'pengolahan', 'teks', 'dan', 'dokumen.']\n"
     ]
    }
   ],
   "source": [
    "# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)\n",
    "text = \"Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen.\"\n",
    "phrases = text.split(',')\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']\n"
     ]
    }
   ],
   "source": [
    "# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia\n",
    "import re\n",
    " \n",
    "text = \"Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan.\"\n",
    "tokens = re.findall(r'\\w+|\\d+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']\n"
     ]
    }
   ],
   "source": [
    "# Misalnya menggunakan spasi sebagai pemisah kata\n",
    "text = \"Ini adalah contoh tokenisasi berbasis model.\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli: running, Kata setelah stemming: run\n",
      "Kata asli: runs, Kata setelah stemming: run\n",
      "Kata asli: runner, Kata setelah stemming: runner\n",
      "Kata asli: ran, Kata setelah stemming: ran\n",
      "Kata asli: easily, Kata setelah stemming: easili\n",
      "Kata asli: fairness, Kata setelah stemming: fair\n",
      "Kata asli: better, Kata setelah stemming: better\n",
      "Kata asli: best, Kata setelah stemming: best\n",
      "Kata asli: cats, Kata setelah stemming: cat\n",
      "Kata asli: cacti, Kata setelah stemming: cacti\n",
      "Kata asli: geese, Kata setelah stemming: gees\n",
      "Kata asli: rocks, Kata setelah stemming: rock\n",
      "Kata asli: oxen, Kata setelah stemming: oxen\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "# Inisialisasi stemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Kata-kata asli\n",
    "words = [\"running\", \"runs\", \"runner\", \"ran\", \"easily\", \"fairness\", \"better\", \"best\", \"cats\", \"cacti\", \"geese\", \"rocks\", \"oxen\"]\n",
    " \n",
    "# Melakukan stemming pada setiap kata\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(f\"Kata asli: {word}, Kata setelah stemming: {stemmed_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pongo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli: Run, Kata setelah lematisasi: run\n",
      "Kata asli: Cat, Kata setelah lematisasi: cat\n",
      "Kata asli: Good, Kata setelah lematisasi: good\n",
      "Kata asli: Goose, Kata setelah lematisasi: goose\n",
      "Kata asli: Rock, Kata setelah lematisasi: rock\n",
      "Kata asli: City, Kata setelah lematisasi: city\n",
      "Kata asli: Big, Kata setelah lematisasi: big\n",
      "Kata asli: Happy, Kata setelah lematisasi: happy\n",
      "Kata asli: Run, Kata setelah lematisasi: run\n",
      "Kata asli: Sleep, Kata setelah lematisasi: sleep\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Download wordnet jika belum di-download\n",
    "nltk.download('wordnet')\n",
    " \n",
    "# Inisialisasi lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Kata-kata asli\n",
    "words = [\"Run\", \"Cat\", \"Good\", \"Goose\", \"Rock\", \"City\", \"Big\", \"Happy\", \"Run\", \"Sleep\"]\n",
    " \n",
    "# Melakukan lematisasi pada setiap kata\n",
    "for word in words:\n",
    "    lemma_word = lemmatizer.lemmatize(word.lower())  # Mengonversi ke huruf kecil untuk memastikan pemrosesan yang konsisten\n",
    "    print(f\"Kata asli: {word}, Kata setelah lematisasi: {lemma_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envsaya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
